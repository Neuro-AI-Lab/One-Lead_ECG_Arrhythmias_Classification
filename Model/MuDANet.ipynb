{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5056,
     "status": "ok",
     "timestamp": 1728233684812,
     "user": {
      "displayName": "BluePearl",
      "userId": "18376352769127660300"
     },
     "user_tz": -540
    },
    "id": "d4mhphOnOKr5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'C:/Users/LJY/Desktop/ECG/Model_fold/data/training_9s.npy'\n",
    "label_path = 'C:/Users/LJY/Desktop/ECG/Model_fold/data/training_9s_label.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1728233684812,
     "user": {
      "displayName": "BluePearl",
      "userId": "18376352769127660300"
     },
     "user_tz": -540
    },
    "id": "uKLcomr0OHhO"
   },
   "outputs": [],
   "source": [
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_layer, out_layer, kernel_size, stride):\n",
    "        super(CNNBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_layer, out_layer, kernel_size=kernel_size, stride=stride, padding='valid')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "\n",
    "        self.GAP = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, num_channels, H = x.size()\n",
    "\n",
    "        # squeeze_x = self.GAP(x).view(batch_size, num_channels)\n",
    "        squeeze_x = self.GAP(x)\n",
    "        squeeze_x = squeeze_x.squeeze(dim=2)\n",
    "        #print('squeeze_shape: ', squeeze_x.shape)\n",
    "\n",
    "        squeeze_x = F.relu(self.fc1(squeeze_x))\n",
    "        squeeze_x = F.sigmoid(self.fc2(squeeze_x))\n",
    "        squeeze_x = squeeze_x.unsqueeze(dim=2)\n",
    "        #print('last shape: ', squeeze_x.shape)\n",
    "\n",
    "        return x * squeeze_x\n",
    "\n",
    "\n",
    "# AttentionBlock Dense(384, 768)과 input_dim용 dense까지 3개를 사용하였다.\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.fc2 = nn.Linear(output_dim, output_dim * 2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention = self.fc1(x)\n",
    "        attention = self.fc2(attention)\n",
    "        # attention = self.fc3(attention)\n",
    "        attention = self.softmax(attention)\n",
    "        # print('x  shape: ', x.shape)\n",
    "        # print('attention shape: ', attention.shape)\n",
    "        x_k = x.permute(0, 2, 1)\n",
    "        K = torch.matmul(x_k, attention)\n",
    "        return K\n",
    "\n",
    "\n",
    "class BiLSTMBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.0):\n",
    "        super(BiLSTMBlock, self).__init__()\n",
    "        self.hidden_size = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, dropout=dropout,\n",
    "                            bidirectional=True)#, batch_first=True\n",
    "\n",
    "    def forward(self, x):\n",
    "        # h_0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)).to(self.device)\n",
    "        # c_0 = Variable(torch.zeros(2*self.num_layers, self.hidden_size)).to(self.device)\n",
    "        x, _ = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, dropout=0.2):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1728233684812,
     "user": {
      "displayName": "BluePearl",
      "userId": "18376352769127660300"
     },
     "user_tz": -540
    },
    "id": "tjFx7lEPN-Ly"
   },
   "outputs": [],
   "source": [
    "# MuDANet 모델 설계\n",
    "class MuDANet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MuDANet, self).__init__()\n",
    "\n",
    "        # 1_1. CNNBlock1 Stream-1 (input_layer, output_layer_kernel_size, stride)\n",
    "        self.cnn_block1_stream1 = CNNBlock(1, 128, 3, 1)\n",
    "        self.se_block1_stream1 = SEBlock(128, 128)\n",
    "\n",
    "        self.cnn_block2_stream1 = CNNBlock(128, 256, 9, 3)\n",
    "        self.se_block2_stream1 = SEBlock(256, 256)\n",
    "\n",
    "        self.cnn_block3_stream1 = CNNBlock(256, 256, 9, 3)\n",
    "        self.se_block3_stream1 = SEBlock(256, 256)\n",
    "        # 2_1. DenseBlock (input_dim, output_dim)\n",
    "        self.dense_block1_stream1 = DenseBlock(297, 32)\n",
    "        self.dense_block2_stream1 = DenseBlock(32, 64)\n",
    "        self.dense_block3_stream1 = DenseBlock(64, 128)\n",
    "\n",
    "        # 3_1. CNNBlock2 Stream-1 (input_layer, output_layer_kernel_size, stride)\n",
    "        self.cnn_block4_stream1 = CNNBlock(256, 128, 3, 1)\n",
    "        self.se_block4_stream1 = SEBlock(128, 128)\n",
    "\n",
    "        self.cnn_block5_stream1 = CNNBlock(128, 256, 9, 3)\n",
    "        self.se_block5_stream1 = SEBlock(256, 256)\n",
    "\n",
    "        self.cnn_block6_stream1 = CNNBlock(256, 256, 9, 3)\n",
    "        self.se_block6_stream1 = SEBlock(256, 256)\n",
    "        # 4_1. DenseBlock (input_dim, output_dim)\n",
    "        self.dense_block4_stream1 = DenseBlock(11, 32)\n",
    "        self.dense_block5_stream1 = DenseBlock(32, 64)\n",
    "        self.dense_block6_stream1 = DenseBlock(64, 128)\n",
    "        # 5_1. AttentionBlock(input_dim, output_dim)\n",
    "        self.attention_block_stream1 = AttentionBlock(128, 384)\n",
    "\n",
    "\n",
    "        # 1_2. CNNBlock1 Stream-2 (input_layer, output_layer_kernel_size, stride)\n",
    "        self.cnn_block1_stream2 = CNNBlock(1, 128, 3, 1)\n",
    "        self.se_block1_stream2 = SEBlock(128, 128)\n",
    "\n",
    "        self.cnn_block2_stream2 = CNNBlock(128, 256, 9, 3)\n",
    "        self.se_block2_stream2 = SEBlock(256, 256)\n",
    "\n",
    "        self.cnn_block3_stream2 = CNNBlock(256, 256, 9, 3)\n",
    "        self.se_block3_stream2 = SEBlock(256, 256)\n",
    "        # 2_2. DenseBlock (input_dim, output_dim)\n",
    "        self.dense_block1_stream2 = DenseBlock(297, 32)\n",
    "        self.dense_block2_stream2 = DenseBlock(32, 64)\n",
    "        self.dense_block3_stream2 = DenseBlock(64, 128)\n",
    "\n",
    "        # 3_2. CNNBlock2 Stream-2 (input_layer, output_layer_kernel_size, stride)\n",
    "        self.cnn_block4_stream2 = CNNBlock(256, 128, 3, 1)\n",
    "        self.se_block4_stream2 = SEBlock(128, 128)\n",
    "\n",
    "        self.cnn_block5_stream2 = CNNBlock(128, 256, 9, 3)\n",
    "        self.se_block5_stream2 = SEBlock(256, 256)\n",
    "\n",
    "        self.cnn_block6_stream2 = CNNBlock(256, 256, 9, 3)\n",
    "        self.se_block6_stream2 = SEBlock(256, 256)\n",
    "        # 4_2. DenseBlock (input_dim, output_dim)\n",
    "        self.dense_block4_stream2 = DenseBlock(11, 32)\n",
    "        self.dense_block5_stream2 = DenseBlock(32, 64)\n",
    "        self.dense_block6_stream2 = DenseBlock(64, 128)\n",
    "        # 5_2. AttentionBlock (input_dim, output_dim)\n",
    "        self.attention_block_stream2 = AttentionBlock(128, 384)\n",
    "\n",
    "\n",
    "        # 6_1. Bi-LSTM Block Stream-1 (input_dim, output_dim)\n",
    "        self.bilstm1_block1 = BiLSTMBlock(768, 128, 1)\n",
    "        self.bilstm1_block2 = BiLSTMBlock(256, 256, 1)\n",
    "        self.bilstm1_block3 = BiLSTMBlock(512, 128, 1)\n",
    "        self.bilstm1_block4 = BiLSTMBlock(256, 256, 1)\n",
    "        self.bilstm1_dropout = nn.Dropout(0.4)\n",
    "        # 6_2. Bi-LSTM Block Stream-2 (input_dim, output_dim)\n",
    "        self.bilstm2_block1 = BiLSTMBlock(768, 128, 1)\n",
    "        self.bilstm2_block2 = BiLSTMBlock(256, 256, 1)\n",
    "        self.bilstm2_block3 = BiLSTMBlock(512, 128, 1)\n",
    "        self.bilstm2_block4 = BiLSTMBlock(256, 256, 1)\n",
    "        self.bilstm2_dropout = nn.Dropout(0.4)\n",
    "\n",
    "\n",
    "        # 7. Fully Cannected (input_dim, output_dim)\n",
    "        self.fc1 = DenseBlock(512, 1024, dropout=0.2)\n",
    "        self.fc2 = DenseBlock(1024, 1024, dropout=0.2)\n",
    "        self.fc3 = DenseBlock(1024, 256, dropout=0.0)\n",
    "        self.final_gap = nn.AdaptiveAvgPool1d(1)\n",
    "        #self\n",
    "        # 8. output (input_dim, output_dim)\n",
    "        self.fc4 = DenseBlock(128, num_classes, dropout=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stream-1\n",
    "            # CNNBlock-1\n",
    "        x1 = self.cnn_block1_stream1(x)\n",
    "        # print('cnn1: ' + str(str(x1.shape)))\n",
    "        x1 = self.se_block1_stream1(x1)\n",
    "        # print('se1: ' + str(str(x1.shape)))\n",
    "        x1 = self.cnn_block2_stream1(x1)\n",
    "        # print('cnn2: ' + str(str(x1.shape)))\n",
    "        x1 = self.se_block2_stream1(x1)\n",
    "        # print('se2: ' + str(x1.shape))\n",
    "        x1 = self.cnn_block3_stream1(x1)\n",
    "        # print('cnn3: ' + str(x1.shape))\n",
    "        x1 = self.se_block3_stream1(x1)\n",
    "        # print('se3: ' + str(x1.shape))\n",
    "\n",
    "        # x1 = torch.flatten(x1, start_dim=1)\n",
    "        # print('flatten1: ' + str(x1.shape))\n",
    "        x1 = self.dense_block1_stream1(x1)\n",
    "        # print('dnn1: ' + str(x1.shape))\n",
    "        x1 = self.dense_block2_stream1(x1)\n",
    "        # print('dnn2: ' + str(x1.shape))\n",
    "        x1 = self.dense_block3_stream1(x1)\n",
    "        # print('dnn3: ' + str(x1.shape))\n",
    "\n",
    "            # CNNBlock-2\n",
    "        # x1 = x1.view(x1.size(0), 1, -1)\n",
    "        # print('reshape1: ' + str(x1.shape))\n",
    "        x1 = self.cnn_block4_stream1(x1)\n",
    "        # print('cnn4: ' + str(x1.shape))\n",
    "        x1 = self.se_block4_stream1(x1)\n",
    "        # print('se4: ' + str(x1.shape))\n",
    "        x1 = self.cnn_block5_stream1(x1)\n",
    "        # print('cnn5: ' + str(x1.shape))\n",
    "        x1 = self.se_block5_stream1(x1)\n",
    "        # print('se5: ' + str(x1.shape))\n",
    "        x1 = self.cnn_block6_stream1(x1)\n",
    "        # print('cnn6: ' + str(x1.shape))\n",
    "        x1 = self.se_block6_stream1(x1)\n",
    "        # print('se6: ' + str(x1.shape))\n",
    "\n",
    "        # x1 = torch.flatten(x1, start_dim=1)\n",
    "        # print('flatten2: ' + str(x1.shape))\n",
    "        x1 = self.dense_block4_stream1(x1)\n",
    "        # print('dnn4: ' + str(x1.shape))\n",
    "        x1 = self.dense_block5_stream1(x1)\n",
    "        # print('dnn5: ' + str(x1.shape))\n",
    "        x1 = self.dense_block6_stream1(x1)\n",
    "        # print('dnn6: ' + str(x1.shape))\n",
    "            # AttentionBlock\n",
    "        x1 = self.attention_block_stream1(x1)\n",
    "        # print('attention: ' + str(x1.shape))\n",
    "\n",
    "\n",
    "        # Stream-2\n",
    "            # CNNBlock-1\n",
    "        x2 = self.cnn_block1_stream2(x)\n",
    "        x2 = self.se_block1_stream2(x2)\n",
    "\n",
    "        x2 = self.cnn_block2_stream2(x2)\n",
    "        x2 = self.se_block2_stream2(x2)\n",
    "\n",
    "        x2 = self.cnn_block3_stream2(x2)\n",
    "        x2 = self.se_block3_stream2(x2)\n",
    "\n",
    "        # x2 = torch.flatten(x2, start_dim=1)\n",
    "        x2 = self.dense_block1_stream2(x2)\n",
    "        x2 = self.dense_block2_stream2(x2)\n",
    "        x2 = self.dense_block3_stream2(x2)\n",
    "\n",
    "            # CNNBlock-2\n",
    "        # x2 = x2.view(x2.size(0), 1, -1)\n",
    "        x2 = self.cnn_block4_stream2(x2)\n",
    "        x2 = self.se_block4_stream2(x2)\n",
    "\n",
    "        x2 = self.cnn_block5_stream2(x2)\n",
    "        x2 = self.se_block5_stream2(x2)\n",
    "\n",
    "        x2 = self.cnn_block6_stream2(x2)\n",
    "        x2 = self.se_block6_stream2(x2)\n",
    "\n",
    "        # x2 = torch.flatten(x2, start_dim=1)\n",
    "        x2 = self.dense_block4_stream2(x2)\n",
    "        x2 = self.dense_block5_stream2(x2)\n",
    "        x2 = self.dense_block6_stream2(x2)\n",
    "            # AttentionBlock\n",
    "        x2 = self.attention_block_stream2(x2)\n",
    "\n",
    "        # model_add\n",
    "        x_fused = torch.add(x1, x2)\n",
    "        # print('fused model: ' + str(x_fused.shape))\n",
    "\n",
    "        x_fused_trans = x_fused.permute(0, 2, 1)\n",
    "        #print('trans: ' + str(x_fused_trans.shape))\n",
    "\n",
    "        # Bi-LSTM-Block-1\n",
    "        x_fused1 = self.bilstm1_block1(x_fused)\n",
    "        # print('biLstm1_block1: ' + str(x_fused1.shape))\n",
    "        # Bi-LSTM-Block-2\n",
    "        x_fused1 = self.bilstm1_block2(x_fused1)\n",
    "        # print('biLstm1_block2: ' + str(x_fused1.shape))\n",
    "        # Bi-LSTM-Block-3\n",
    "        x_fused1 = self.bilstm1_block3(x_fused1)\n",
    "        # print('biLstm1_block3: ' + str(x_fused1.shape))\n",
    "        # Bi-LSTM-Block-4\n",
    "        x_fused1 = self.bilstm1_block4(x_fused1)\n",
    "        # print('biLstm1_block4: ' + str(x_fused1.shape))\n",
    "\n",
    "        x_fused1 = self.bilstm1_dropout(x_fused1)\n",
    "\n",
    "        # Bi-LSTM-Block-1\n",
    "        x_fused2 = self.bilstm2_block1(x_fused)\n",
    "        # print('biLstm2_block1: ' + str(x_fused2.shape))\n",
    "        # Bi-LSTM-Block-2\n",
    "        x_fused2 = self.bilstm2_block2(x_fused2)\n",
    "        # print('biLstm2_block2: ' + str(x_fused2.shape))\n",
    "        # Bi-LSTM-Block-3\n",
    "        x_fused2 = self.bilstm2_block3(x_fused2)\n",
    "        # print('biLstm2_block3: ' + str(x_fused2.shape))\n",
    "        # Bi-LSTM-Block-4\n",
    "        x_fused2 = self.bilstm2_block4(x_fused2)\n",
    "        #print('biLstm2_block4: ' + str(x_fused2.shape))\n",
    "\n",
    "        x_fused2 = self.bilstm2_dropout(x_fused2)\n",
    "\n",
    "\n",
    "        x_final_fusion_trans = x_fused1 + x_fused2\n",
    "        # model_add\n",
    "        #x_fused = torch.add(x_fused1, x_fused2)\n",
    "        #print('fused: ' + str(x_fused.shape))\n",
    "        # Fully_Connected\n",
    "        #x_final_fusion_trans = x_final_fusion.permute(0, 2, 1)\n",
    "        #print('trans: ' + str(x_final_fusion_trans.shape))\n",
    "        x_final_fusion_trans = self.fc1(x_final_fusion_trans)\n",
    "        # print('Final fc1: ' + str(x_final_fusion_trans.shape))\n",
    "        x_final_fusion_trans = self.fc2(x_final_fusion_trans)\n",
    "        # print('Final fc2: ' + str(x_final_fusion_trans.shape))\n",
    "        x_final_fusion_trans = self.fc3(x_final_fusion_trans)\n",
    "        #print('Final fc3: ' + str(x_final_fusion_trans.shape))\n",
    "\n",
    "        # Output\n",
    "        x_final_fusion_trans = self.final_gap(x_final_fusion_trans)\n",
    "        #print('Final GAP: ' + str(x_final_fusion_trans.shape))\n",
    "        x_final_fusion_trans = x_final_fusion_trans.squeeze(2)\n",
    "        output = self.fc4(x_final_fusion_trans)\n",
    "        # print('Final fc4: ' + str(output.shape))\n",
    "\n",
    "        return F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "executionInfo": {
     "elapsed": 122200,
     "status": "error",
     "timestamp": 1728233883877,
     "user": {
      "displayName": "BluePearl",
      "userId": "18376352769127660300"
     },
     "user_tz": -540
    },
    "id": "YTl5LnedOSOY",
    "outputId": "935dd4a8-5cf2-4121-b3ad-c087e47b2a50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LJY\\AppData\\Local\\Temp\\ipykernel_8032\\1765303083.py:238: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(output)\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device='cuda'):\n",
    "    model = model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model = model.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_accuracy = correct / total\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        #fold_accuracy.append(val_accuracy)\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device='cuda'):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            model = model.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct / total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "\n",
    "class ECGDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "def main():\n",
    "    np_data = np.load(data_path).astype(np.float32)\n",
    "    label = np.load(label_path).astype(np.float32)\n",
    "\n",
    "\n",
    "    data = torch.tensor(np_data.reshape(np_data.shape[0], 1, np_data.shape[2]), dtype=torch.float32)\n",
    "    labels = torch.tensor(label, dtype=torch.long)\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.1)\n",
    "\n",
    "    train_dataset = ECGDataset(train_data, train_labels)\n",
    "    val_dataset = ECGDataset(val_data, val_labels)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print('device', device)\n",
    "\n",
    "    model = MuDANet(num_classes=3)\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=40, device=device)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNQ6PpxlgyPB0Rhr3yoopDr",
   "gpuType": "A100",
   "machine_shape": "hm",
   "mount_file_id": "1CCJWeKuWXof8PS0YSNbt1E35Y5MqemnL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
