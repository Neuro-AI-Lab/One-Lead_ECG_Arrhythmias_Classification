{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","machine_shape":"hm","mount_file_id":"1QoLqBMaPFD61oNUdEGXYSCbDdKmAqeOb","authorship_tag":"ABX9TyNLxoaN5BvaeYlJp0PQwY+A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score\n","from torch.autograd import Variable\n","from tqdm import tqdm\n","from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve, auc, matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score, average_precision_score, brier_score_loss, log_loss, precision_recall_curve, average_precision_score, multilabel_confusion_matrix\n","import matplotlib.pyplot as plt"],"metadata":{"id":"d4mhphOnOKr5","executionInfo":{"status":"ok","timestamp":1728346368222,"user_tz":-540,"elapsed":583,"user":{"displayName":"BluePearl","userId":"18376352769127660300"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class CNNBlock(nn.Module):\n","    def __init__(self, in_layer, out_layer, kernel_size, stride):\n","        super(CNNBlock, self).__init__()\n","        # Loss를 줄이기위해 BatchNorm1d를 사용함\n","        self.conv1 = nn.Conv1d(in_layer, out_layer, kernel_size=kernel_size, stride=stride, padding='valid')\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","\n","        return x\n","\n","class SEBlock(nn.Module):\n","    def __init__(self, in_channels, reduction=16):\n","        super(SEBlock, self).__init__()\n","\n","        self.GAP = nn.AdaptiveAvgPool1d(1)\n","        self.fc1 = nn.Linear(in_channels, in_channels // reduction)\n","        self.fc2 = nn.Linear(in_channels // reduction, in_channels)\n","\n","    def forward(self, x):\n","        batch_size, num_channels, H = x.size()\n","\n","        # squeeze_x = self.GAP(x).view(batch_size, num_channels)\n","        squeeze_x = self.GAP(x)\n","        squeeze_x = squeeze_x.squeeze(dim=2)\n","        #print('squeeze_shape: ', squeeze_x.shape)\n","\n","        squeeze_x = F.relu(self.fc1(squeeze_x))\n","        squeeze_x = F.sigmoid(self.fc2(squeeze_x))\n","        squeeze_x = squeeze_x.unsqueeze(dim=2)\n","        #print('last shape: ', squeeze_x.shape)\n","\n","        return x * squeeze_x\n","\n","\n","# AttentionBlock Dense(384, 768)과 input_dim용 dense까지 3개를 사용하였다.\n","class AttentionBlock(nn.Module):\n","    def __init__(self, input_dim, output_dim):\n","        super(AttentionBlock, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, output_dim)\n","        self.fc2 = nn.Linear(output_dim, output_dim * 2)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        attention = self.fc1(x)\n","        attention = self.fc2(attention)\n","        # attention = self.fc3(attention)\n","        attention = self.softmax(attention)\n","        # print('x  shape: ', x.shape)\n","        # print('attention shape: ', attention.shape)\n","        x_k = x.permute(0, 2, 1)\n","        K = torch.matmul(x_k, attention)\n","        return K\n","\n","\n","class BiLSTMBlock(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, num_layers, dropout=0.0):\n","        super(BiLSTMBlock, self).__init__()\n","        self.hidden_size = hidden_dim\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, dropout=dropout,\n","                            bidirectional=True)#, batch_first=True\n","\n","    def forward(self, x):\n","        # h_0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)).to(self.device)\n","        # c_0 = Variable(torch.zeros(2*self.num_layers, self.hidden_size)).to(self.device)\n","        x, _ = self.lstm(x)\n","        return x\n","\n","\n","class DenseBlock(nn.Module):\n","    def __init__(self, input_dim, output_dim, dropout=0.2):\n","        super(DenseBlock, self).__init__()\n","        self.fc1 = nn.Linear(input_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.dropout(x)\n","        return x"],"metadata":{"id":"uKLcomr0OHhO","executionInfo":{"status":"ok","timestamp":1728346368773,"user_tz":-540,"elapsed":4,"user":{"displayName":"BluePearl","userId":"18376352769127660300"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"tjFx7lEPN-Ly","executionInfo":{"status":"ok","timestamp":1728346370259,"user_tz":-540,"elapsed":2,"user":{"displayName":"BluePearl","userId":"18376352769127660300"}}},"outputs":[],"source":["# MuDANet 모델 설계\n","class MuDANet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(MuDANet, self).__init__()\n","\n","        # 1_1. CNNBlock1 Stream-1 (input_layer, output_layer_kernel_size, stride)\n","        self.cnn_block1_stream1 = CNNBlock(1, 128, 3, 1)\n","        self.se_block1_stream1 = SEBlock(128, 128)\n","\n","        self.cnn_block2_stream1 = CNNBlock(128, 256, 9, 3)\n","        self.se_block2_stream1 = SEBlock(256, 256)\n","\n","        self.cnn_block3_stream1 = CNNBlock(256, 256, 9, 3)\n","        self.se_block3_stream1 = SEBlock(256, 256)\n","        # 2_1. DenseBlock (input_dim, output_dim)\n","        self.dense_block1_stream1 = DenseBlock(297, 32)\n","        self.dense_block2_stream1 = DenseBlock(32, 64)\n","        self.dense_block3_stream1 = DenseBlock(64, 128)\n","\n","        # 3_1. CNNBlock2 Stream-1 (input_layer, output_layer_kernel_size, stride)\n","        self.cnn_block4_stream1 = CNNBlock(256, 128, 3, 1)\n","        self.se_block4_stream1 = SEBlock(128, 128)\n","\n","        self.cnn_block5_stream1 = CNNBlock(128, 256, 9, 3)\n","        self.se_block5_stream1 = SEBlock(256, 256)\n","\n","        self.cnn_block6_stream1 = CNNBlock(256, 256, 9, 3)\n","        self.se_block6_stream1 = SEBlock(256, 256)\n","        # 4_1. DenseBlock (input_dim, output_dim)\n","        self.dense_block4_stream1 = DenseBlock(11, 32)\n","        self.dense_block5_stream1 = DenseBlock(32, 64)\n","        self.dense_block6_stream1 = DenseBlock(64, 128)\n","        # 5_1. AttentionBlock(input_dim, output_dim)\n","        self.attention_block_stream1 = AttentionBlock(128, 384)\n","\n","\n","        # 1_2. CNNBlock1 Stream-2 (input_layer, output_layer_kernel_size, stride)\n","        self.cnn_block1_stream2 = CNNBlock(1, 128, 3, 1)\n","        self.se_block1_stream2 = SEBlock(128, 128)\n","\n","        self.cnn_block2_stream2 = CNNBlock(128, 256, 9, 3)\n","        self.se_block2_stream2 = SEBlock(256, 256)\n","\n","        self.cnn_block3_stream2 = CNNBlock(256, 256, 9, 3)\n","        self.se_block3_stream2 = SEBlock(256, 256)\n","        # 2_2. DenseBlock (input_dim, output_dim)\n","        self.dense_block1_stream2 = DenseBlock(297, 32)\n","        self.dense_block2_stream2 = DenseBlock(32, 64)\n","        self.dense_block3_stream2 = DenseBlock(64, 128)\n","\n","        # 3_2. CNNBlock2 Stream-2 (input_layer, output_layer_kernel_size, stride)\n","        self.cnn_block4_stream2 = CNNBlock(256, 128, 3, 1)\n","        self.se_block4_stream2 = SEBlock(128, 128)\n","\n","        self.cnn_block5_stream2 = CNNBlock(128, 256, 9, 3)\n","        self.se_block5_stream2 = SEBlock(256, 256)\n","\n","        self.cnn_block6_stream2 = CNNBlock(256, 256, 9, 3)\n","        self.se_block6_stream2 = SEBlock(256, 256)\n","        # 4_2. DenseBlock (input_dim, output_dim)\n","        self.dense_block4_stream2 = DenseBlock(11, 32)\n","        self.dense_block5_stream2 = DenseBlock(32, 64)\n","        self.dense_block6_stream2 = DenseBlock(64, 128)\n","        # 5_2. AttentionBlock (input_dim, output_dim)\n","        self.attention_block_stream2 = AttentionBlock(128, 384)\n","\n","\n","        # 6_1. Bi-LSTM Block Stream-1 (input_dim, output_dim)\n","        self.bilstm1_block1 = BiLSTMBlock(128, 128, 1)\n","        self.bilstm1_block2 = BiLSTMBlock(256, 256, 1)\n","        self.bilstm1_block3 = BiLSTMBlock(512, 128, 1)\n","        self.bilstm1_block4 = BiLSTMBlock(256, 256, 1)\n","        self.bilstm1_dropout = nn.Dropout(0.4)\n","        # 6_2. Bi-LSTM Block Stream-2 (input_dim, output_dim)\n","        self.bilstm2_block1 = BiLSTMBlock(128, 128, 1)\n","        self.bilstm2_block2 = BiLSTMBlock(256, 256, 1)\n","        self.bilstm2_block3 = BiLSTMBlock(512, 128, 1)\n","        self.bilstm2_block4 = BiLSTMBlock(256, 256, 1)\n","        self.bilstm2_dropout = nn.Dropout(0.4)\n","\n","\n","        # 7. Fully Cannected (input_dim, output_dim)\n","        self.fc1 = DenseBlock(512, 1024, dropout=0.2)\n","        self.fc2 = DenseBlock(1024, 1024, dropout=0.2)\n","        self.fc3 = DenseBlock(1024, 256, dropout=0.0)\n","        self.final_gap = nn.AdaptiveAvgPool1d(1)\n","        #self\n","        # 8. output (input_dim, output_dim)\n","        self.fc4 = DenseBlock(256, num_classes, dropout=0.0)\n","\n","    def forward(self, x):\n","        # Stream-1\n","            # CNNBlock-1\n","        x1 = self.cnn_block1_stream1(x)\n","        # print('cnn1: ' + str(str(x1.shape)))\n","        x1 = self.se_block1_stream1(x1)\n","        # print('se1: ' + str(str(x1.shape)))\n","        x1 = self.cnn_block2_stream1(x1)\n","        # print('cnn2: ' + str(str(x1.shape)))\n","        x1 = self.se_block2_stream1(x1)\n","        # print('se2: ' + str(x1.shape))\n","        x1 = self.cnn_block3_stream1(x1)\n","        # print('cnn3: ' + str(x1.shape))\n","        x1 = self.se_block3_stream1(x1)\n","        # print('se3: ' + str(x1.shape))\n","\n","        # x1 = torch.flatten(x1, start_dim=1)\n","        # print('flatten1: ' + str(x1.shape))\n","        x1 = self.dense_block1_stream1(x1)\n","        # print('dnn1: ' + str(x1.shape))\n","        x1 = self.dense_block2_stream1(x1)\n","        # print('dnn2: ' + str(x1.shape))\n","        x1 = self.dense_block3_stream1(x1)\n","        # print('dnn3: ' + str(x1.shape))\n","\n","            # CNNBlock-2\n","        # x1 = x1.view(x1.size(0), 1, -1)\n","        # print('reshape1: ' + str(x1.shape))\n","        x1 = self.cnn_block4_stream1(x1)\n","        # print('cnn4: ' + str(x1.shape))\n","        x1 = self.se_block4_stream1(x1)\n","        # print('se4: ' + str(x1.shape))\n","        x1 = self.cnn_block5_stream1(x1)\n","        # print('cnn5: ' + str(x1.shape))\n","        x1 = self.se_block5_stream1(x1)\n","        # print('se5: ' + str(x1.shape))\n","        x1 = self.cnn_block6_stream1(x1)\n","        # print('cnn6: ' + str(x1.shape))\n","        x1 = self.se_block6_stream1(x1)\n","        # print('se6: ' + str(x1.shape))\n","\n","        # x1 = torch.flatten(x1, start_dim=1)\n","        # print('flatten2: ' + str(x1.shape))\n","        x1 = self.dense_block4_stream1(x1)\n","        # print('dnn4: ' + str(x1.shape))\n","        x1 = self.dense_block5_stream1(x1)\n","        # print('dnn5: ' + str(x1.shape))\n","        x1 = self.dense_block6_stream1(x1)\n","        # print('dnn6: ' + str(x1.shape))\n","            # AttentionBlock\n","        #x1 = self.attention_block_stream1(x1)\n","        # print('attention: ' + str(x1.shape))\n","\n","\n","        # Stream-2\n","            # CNNBlock-1\n","        x2 = self.cnn_block1_stream2(x)\n","        x2 = self.se_block1_stream2(x2)\n","\n","        x2 = self.cnn_block2_stream2(x2)\n","        x2 = self.se_block2_stream2(x2)\n","\n","        x2 = self.cnn_block3_stream2(x2)\n","        x2 = self.se_block3_stream2(x2)\n","\n","        # x2 = torch.flatten(x2, start_dim=1)\n","        x2 = self.dense_block1_stream2(x2)\n","        x2 = self.dense_block2_stream2(x2)\n","        x2 = self.dense_block3_stream2(x2)\n","\n","            # CNNBlock-2\n","        # x2 = x2.view(x2.size(0), 1, -1)\n","        x2 = self.cnn_block4_stream2(x2)\n","        x2 = self.se_block4_stream2(x2)\n","\n","        x2 = self.cnn_block5_stream2(x2)\n","        x2 = self.se_block5_stream2(x2)\n","\n","        x2 = self.cnn_block6_stream2(x2)\n","        x2 = self.se_block6_stream2(x2)\n","\n","        # x2 = torch.flatten(x2, start_dim=1)\n","        x2 = self.dense_block4_stream2(x2)\n","        x2 = self.dense_block5_stream2(x2)\n","        x2 = self.dense_block6_stream2(x2)\n","            # AttentionBlock\n","        #x2 = self.attention_block_stream2(x2)\n","\n","        # model_add\n","        x_fused = torch.add(x1, x2)\n","        #print('fused model: ' + str(x_fused.shape))\n","\n","        x_fused_trans = x_fused.permute(0, 2, 1)\n","        #print('trans: ' + str(x_fused_trans.shape))\n","\n","        # Bi-LSTM-Block-1\n","        x_fused1 = self.bilstm1_block1(x_fused)\n","        # print('biLstm1_block1: ' + str(x_fused1.shape))\n","        # Bi-LSTM-Block-2\n","        x_fused1 = self.bilstm1_block2(x_fused1)\n","        # print('biLstm1_block2: ' + str(x_fused1.shape))\n","        # Bi-LSTM-Block-3\n","        x_fused1 = self.bilstm1_block3(x_fused1)\n","        # print('biLstm1_block3: ' + str(x_fused1.shape))\n","        # Bi-LSTM-Block-4\n","        x_fused1 = self.bilstm1_block4(x_fused1)\n","        # print('biLstm1_block4: ' + str(x_fused1.shape))\n","\n","        x_fused1 = self.bilstm1_dropout(x_fused1)\n","\n","        # Bi-LSTM-Block-1\n","        x_fused2 = self.bilstm2_block1(x_fused)\n","        # print('biLstm2_block1: ' + str(x_fused2.shape))\n","        # Bi-LSTM-Block-2\n","        x_fused2 = self.bilstm2_block2(x_fused2)\n","        # print('biLstm2_block2: ' + str(x_fused2.shape))\n","        # Bi-LSTM-Block-3\n","        x_fused2 = self.bilstm2_block3(x_fused2)\n","        # print('biLstm2_block3: ' + str(x_fused2.shape))\n","        # Bi-LSTM-Block-4\n","        x_fused2 = self.bilstm2_block4(x_fused2)\n","        #print('biLstm2_block4: ' + str(x_fused2.shape))\n","\n","        x_fused2 = self.bilstm2_dropout(x_fused2)\n","\n","\n","        x_final_fusion_trans = x_fused1 + x_fused2\n","        # model_add\n","        #x_fused = torch.add(x_fused1, x_fused2)\n","        #print('fused: ' + str(x_fused.shape))\n","        # Fully_Connected\n","        #x_final_fusion_trans = x_final_fusion.permute(0, 2, 1)\n","        #print('trans: ' + str(x_final_fusion_trans.shape))\n","        x_final_fusion_trans = self.fc1(x_final_fusion_trans)\n","        #print('Final fc1: ' + str(x_final_fusion_trans.shape))\n","        x_final_fusion_trans = self.fc2(x_final_fusion_trans)\n","        #print('Final fc2: ' + str(x_final_fusion_trans.shape))\n","        x_final_fusion_trans = self.fc3(x_final_fusion_trans)\n","        #print('Final fc3: ' + str(x_final_fusion_trans.shape))\n","\n","        # Output\n","        x_final_fusion_trans = self.final_gap(x_final_fusion_trans)\n","        #print('Final GAP: ' + str(x_final_fusion_trans.shape))\n","        x_final_fusion_trans = x_final_fusion_trans.squeeze(2)\n","        #print('x_final_fusion_trans: ' + str(x_final_fusion_trans.shape))\n","        output = self.fc4(x_final_fusion_trans)\n","        #print('Final fc4: ' + str(output.shape))\n","\n","        return F.softmax(output)"]},{"cell_type":"markdown","source":["# 10-Fold & metrics"],"metadata":{"id":"6-yep6M9orEJ"}},{"cell_type":"code","source":["fold_accuracy = []\n","def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device='cuda'):\n","    model = model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_accuracy = correct / total\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n","\n","        val_loss, val_accuracy = evaluate_model(model, val_loader,criterion, device)\n","        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","        fold_accuracy.append(val_accuracy)\n","\n","\n","def evaluate_model(model, val_loader, criterion, device='cuda'):\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","    label_1d = []\n","    predicted_1d = []\n","    # metrics = {label: {'TP': 0, 'FP': 0, 'FN': 0, 'TN' : 0} for label in range(3)}\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            model = model.to(device)\n","            outputs = model(inputs)\n","\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            label_1d.append(labels.clone().detach().cpu())\n","            predicted_1d.append(predicted.clone().detach().cpu())\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    label_1d = np.concatenate(label_1d).tolist()\n","    predicted_1d = np.concatenate(predicted_1d).tolist()\n","    print(classification_report(label_1d, predicted_1d, zero_division = 0, digits=4))\n","    val_loss /= len(val_loader.dataset)\n","    val_accuracy = correct / total\n","    return val_loss, val_accuracy\n","\n","class ECGDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx], self.labels[idx]\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv1d):\n","        nn.init.xavier_uniform_(m.weight)\n","\n","def main():\n","    np_data = np.load('/content/drive/MyDrive/ECG/training_9s_k.npy').astype(np.float32)\n","    label = np.load('/content/drive/MyDrive/ECG/training_9s_label_k.npy').astype(np.float32)\n","\n","\n","    X_tensor = torch.tensor(np_data.reshape(np_data.shape[0], 1, np_data.shape[2]), dtype=torch.float32)\n","    y_tensor = torch.tensor(label, dtype=torch.long)\n","    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n","\n","    for fold, (train_index, test_index) in enumerate(kf.split(X_tensor)):\n","        print(f\"Fold {fold + 1}\")\n","        # Train/test 데이터 나누기\n","        X_train, X_test = X_tensor[train_index], X_tensor[test_index]\n","        y_train, y_test = y_tensor[train_index], y_tensor[test_index]\n","\n","        # 데이터 로더 생성\n","        train_dataset = ECGDataset(X_train, y_train)\n","        test_dataset = ECGDataset(X_test, y_test)\n","\n","        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","        val_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","        model = MuDANet(num_classes=3)\n","        model.apply(weights_init)\n","\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=40, device=device)\n","\n","if __name__ == '__main__':\n","    main()\n","    print(f\"\\nOverall Accuracy: {np.mean(fold_accuracy):.4f}\")"],"metadata":{"id":"Pw8kTUlEwf7t","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1728352892673,"user_tz":-540,"elapsed":91294,"user":{"displayName":"BluePearl","userId":"18376352769127660300"}},"outputId":"4404868b-7102-4f13-8eda-d829918ea824"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-c9fd89ba6b76>:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(output)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [1/40], Loss: 0.9602, Accuracy: 0.5927\n","              precision    recall  f1-score   support\n","\n","           0     0.5863    1.0000    0.7392      1546\n","           1     0.0000    0.0000    0.0000       234\n","           2     0.0000    0.0000    0.0000       857\n","\n","    accuracy                         0.5863      2637\n","   macro avg     0.1954    0.3333    0.2464      2637\n","weighted avg     0.3437    0.5863    0.4334      2637\n","\n","Validation Loss: 0.9652, Validation Accuracy: 0.5863\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-c9fd89ba6b76>:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(output)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [2/40], Loss: 0.9552, Accuracy: 0.5962\n","              precision    recall  f1-score   support\n","\n","           0     0.5863    1.0000    0.7392      1546\n","           1     0.0000    0.0000    0.0000       234\n","           2     0.0000    0.0000    0.0000       857\n","\n","    accuracy                         0.5863      2637\n","   macro avg     0.1954    0.3333    0.2464      2637\n","weighted avg     0.3437    0.5863    0.4334      2637\n","\n","Validation Loss: 0.9652, Validation Accuracy: 0.5863\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-c9fd89ba6b76>:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(output)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch [3/40], Loss: 0.9552, Accuracy: 0.5962\n","              precision    recall  f1-score   support\n","\n","           0     0.5863    1.0000    0.7392      1546\n","           1     0.0000    0.0000    0.0000       234\n","           2     0.0000    0.0000    0.0000       857\n","\n","    accuracy                         0.5863      2637\n","   macro avg     0.1954    0.3333    0.2464      2637\n","weighted avg     0.3437    0.5863    0.4334      2637\n","\n","Validation Loss: 0.9652, Validation Accuracy: 0.5863\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-9-c9fd89ba6b76>:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(output)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-b55a2647e1b9>\u001b[0m in \u001b[0;36m<cell line: 110>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nOverall Accuracy: {np.mean(fold_accuracy):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-b55a2647e1b9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-21-b55a2647e1b9>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                             )\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    224\u001b[0m             )\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    227\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    767\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"WDaVq-yxOVUM","colab":{"base_uri":"https://localhost:8080/","height":147},"executionInfo":{"status":"error","timestamp":1728233749741,"user_tz":-540,"elapsed":583,"user":{"displayName":"BluePearl","userId":"18376352769127660300"}},"outputId":"52f983a9-e489-4ff1-9778-47fec33d3b60"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"tuple index out of range","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-91ea1e86a579>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"cell_type":"code","source":["def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=30, device='cuda'):\n","    model = model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","        correct = 0\n","        total = 0\n","\n","        for inputs, labels in train_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            model = model.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","        train_loss = running_loss / len(train_loader.dataset)\n","        train_accuracy = correct / total\n","        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n","\n","        val_loss, val_accuracy = evaluate_model(model, val_loader, criterion, device)\n","        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n","        #fold_accuracy.append(val_accuracy)\n","\n","'''\n","def evaluate_model(model, val_loader, criterion, device='cuda'):\n","    model.eval()\n","    val_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            model = model.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            val_loss += loss.item() * inputs.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    val_loss /= len(val_loader.dataset)\n","    val_accuracy = correct / total\n","    return val_loss, val_accuracy\n","'''\n","def test(t_model, dataloader, device):\n","    t_model.eval()\n","    total_loss = 0.0\n","    metrics = {label: {'TP': 0, 'FP': 0, 'FN': 0, 'TN' : 0} for label in range(3)}\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Testing\"):\n","            x, y = batch\n","            x, y = x.to(device), y.to(device)\n","\n","            seg, out = t_model.forward_features(x)\n","            predictions = torch.argmax(seg, dim=1)\n","            batch_metrics = calculate_metrics_per_class(predictions, y, num_classes=3)\n","            for label in range(3):\n","                metrics[label]['TP'] += batch_metrics[label]['TP']\n","                metrics[label]['FP'] += batch_metrics[label]['FP']\n","                metrics[label]['FN'] += batch_metrics[label]['FN']\n","                metrics[label]['TN'] += batch_metrics[label]['TN']\n","            # Calculate loss\n","            loss = t_model.calculate_loss(y, seg)\n","            total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(dataloader)\n","    print(f\"Test Loss: {avg_loss}\")\n","\n","    # 최종 메트릭 계산\n","    final_metrics = calculate_final_metrics(metrics, num_classes=3)\n","\n","    # 각 클래스의 Precision, Recall, F1-Score, IOU 출력\n","    for label, metric in final_metrics.items():\n","        label_name = \"Label Normal\" if label == 0 else \"Label AFL\" if label == 1 else \"Label AFIB\" if label == 2 else \"Label Others\"\n","        print(f\"{label_name}: Precision: {metric['Precision']:.4f} / Recall: {metric['Recall']:.4f} / F1-Score: {metric['F1-Score']:.4f} / IOU: {metric['IOU']:.4f} / Acc: {metric['Accuracy']:.4f}\")\n","\n","    return avg_loss, final_metrics\n","\n","def calculate_metrics_per_class(predictions, ground_truth, num_classes=3):\n","    metrics = {label: {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0} for label in range(num_classes)}\n","    for label in range(num_classes):\n","        TP = ((predictions == label) & (ground_truth == label)).sum().item()\n","        FP = ((predictions == label) & (ground_truth != label)).sum().item()\n","        FN = ((predictions != label) & (ground_truth == label)).sum().item()\n","        TN = ((predictions != label) & (ground_truth != label)).sum().item()  # True Negative 계산\n","\n","        metrics[label]['TP'] += TP\n","        metrics[label]['FP'] += FP\n","        metrics[label]['FN'] += FN\n","        metrics[label]['TN'] += TN\n","    return metrics\n","\n","def calculate_final_metrics(metrics, num_classes=4):\n","    results = {}\n","    for label in range(num_classes):\n","        TP = metrics[label]['TP']\n","        FP = metrics[label]['FP']\n","        FN = metrics[label]['FN']\n","        TN = metrics[label]['TN']\n","        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","        iou = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n","        accuracy = (TP + TN) / (TP + TN + FP + FN) if (TP + TN + FP + FN) > 0 else 0  # Accuracy 계산\n","\n","        results[label] = {\n","            'Precision': precision,\n","            'Recall': recall,\n","            'F1-Score': f1_score,\n","            'IOU': iou,\n","            'Accuracy': accuracy  # Accuracy 추가\n","        }\n","    return results\n","class ECGDataset(Dataset):\n","    def __init__(self, data, labels):\n","        self.data = data\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        return self.data[idx], self.labels[idx]\n","\n","def weights_init(m):\n","    if isinstance(m, nn.Conv1d):\n","        nn.init.xavier_uniform_(m.weight)\n","\n","def main():\n","    np_data = np.load('/content/drive/MyDrive/ECG/training_9s_k.npy').astype(np.float32)\n","    label = np.load('/content/drive/MyDrive/ECG/training_9s_label_k.npy').astype(np.float32)\n","\n","\n","    data = torch.tensor(np_data.reshape(np_data.shape[0], 1, np_data.shape[2]), dtype=torch.float32)\n","    labels = torch.tensor(label, dtype=torch.long)\n","    train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.1)\n","\n","    train_dataset = ECGDataset(train_data, train_labels)\n","    val_dataset = ECGDataset(val_data, val_labels)\n","\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print('device', device)\n","\n","    model = MuDANet(num_classes=3)\n","    model.apply(weights_init)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","\n","\n","    train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=40, device=device)\n","\n","if __name__ == '__main__':\n","    main()\n","    print(f\"\\nOverall Accuracy: {np.mean(fold_accuracy):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"YTl5LnedOSOY","outputId":"fdafe459-0d27-4e27-86c8-be271b927812","executionInfo":{"status":"error","timestamp":1728342957333,"user_tz":-540,"elapsed":13881,"user":{"displayName":"BluePearl","userId":"18376352769127660300"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["device cuda\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-4-c9fd89ba6b76>:239: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  return F.softmax(output)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-e6a7196b6520>\u001b[0m in \u001b[0;36m<cell line: 169>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nOverall Accuracy: {np.mean(fold_accuracy):.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-e6a7196b6520>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-e6a7196b6520>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-c9fd89ba6b76>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;31m# print('biLstm2_block3: ' + str(x_fused2.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Bi-LSTM-Block-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mx_fused2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm2_block4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fused2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;31m#print('biLstm2_block4: ' + str(x_fused2.shape))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-db158b50290f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# h_0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size)).to(self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# c_0 = Variable(torch.zeros(2*self.num_layers, self.hidden_size)).to(self.device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    918\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}